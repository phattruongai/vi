---
layout: post
title: Xây dựng CNN phân loại cà chua , trực quan hóa dự đoán bằng Grad-CAM, nhúng trên Raspberry
description: DATN
summary: |-
    Đây là đồ án tốt nghiệp của mình dưới sự hướng dẫn của thầy Vũ Quang Huy, Đại học Sư Phạm Kỹ Thuật TPHCM. Trong bài hướng dẫn này thì mình sẽ giới thiệu chủ yếu về cách mình xây dựng một mô hình phân loại ảnh (image classification model) và chạy hệ thống trên raspberry Pi trong thời gian thực.

    Mình cũng đã publish một [IEEE paper](https://ieeexplore.ieee.org/document/9303079) về mạng neural này và được trình bày tại hội nghị [GTSD 2020](https://easychair.org/cfp/GTSD2020)


date: 2020-05-20 18:00:00
image: /img/DATN/model.png
tags: projects realtime deep-learning computer-vision
comments: true
toc: true
#toc_title: Custom Title
menubar: 
show_sidebar: true
---

---
## 1. Tổng quan

{% include message.html type="is-info" content="Đây là đồ án tốt nghiệp của mình dưới sự hướng dẫn của thầy Vũ Quang Huy, Đại học Sư Phạm Kỹ Thuật TPHCM. Trong bài hướng dẫn này thì mình sẽ giới thiệu chủ yếu về cách mình xây dựng một mô hình phân loại ảnh (image classification model) và chạy hệ thống trên raspberry Pi trong thời gian thực." %}

{% include message.html type="is-info" content="Mình cũng đã publish một <a href='https://ieeexplore.ieee.org/document/9303079'>IEEE paper</a> về mạng neural này và được trình bày tại hội nghị <a href='https://easychair.org/cfp/GTSD2020'>GTSD 2020</a>." %}

<div class="has-text-centered">
    <img src="/img/DATN/3D.png">
    <img src="/img/DATN/tSNE.png" alt="drawing" width="200"/>
    <img src="/img/DATN/model.png" alt="drawing" width="700" height="200"/>
</div>

--- 

## 2. Chuẩn bị dữ liệu

{% include message.html type="is-warning" content="Download dữ liệu tại <a href='https://drive.google.com/file/d/1iYhwA9zTVz6qtfM-tpNlkRhuwxPcWnPr/view?usp=sharing'>đây</a>" %}

### Clone github về local machine của mình:

* Chạy: 

```
git clone https://github.com/phattruongai/Tomato-classification-system.git
cd Tomato-classification-system/Classification
```

### Tổ chức thư mục 

Sau khi download dữ liệu về và giải nén thì ta sẽ có 1 thư mục tên `cachua_resize2`, đặt thư mục này như đường dẫn sau `Tomato-classification-system/Classification/cachua_resize2`

* Cây thư mục:

```
Classification
    |---cachua_resize2
        |---train
        |   |---loai1
        |   |---loai2
        |   |---loai3
        |   |---loai4
        |   |---loai5
        |   `---loai6
        `---test
            |---loai1
            |---loai2
            |---loai3
            |---loai4
            |---loai5
            `---loai6
```

### Tiền xử lí và đóng gói dữ liệu:

Mở file `Classification/make_data_and_preprocessing/make_data/make_data/make_data.py` để xem các đoạn code, mình sẽ giải thích một số đoạn, để đơn giản, các bạn chỉ cần chạy script như cuối phần này

* Resize và normalize:

{% include message.html type="is-info" content="Tất cả các ảnh được đưa về 100x100 và chia 255 để có giá trị nằm trong khoảng [0,1]" %}

```
train_classes = os.listdir(train_dir)
x_train = []
y_train = []
i = 0
for t_class in train_classes:
    y_now = np.zeros((6,))
    t_list = os.listdir(train_dir+t_class)
    y_now[int(t_class[4])-1] = 1
    for img_name in t_list:
        print(t_class,img_name)
        img = cv2.imread(train_dir+t_class+"/"+img_name)
        img = cv2.resize(img,(100,100)) #resize
        img = img/255 #normalization
        x_train.append(img)
        y_train.append(y_now)
```

* Đóng gói bằng thư viện h5py:

Sau khi tạo ra được mảng x_train và y_train dưới dạng numpy.array, ta lưu vào file h5py để tiện việc sử dụng nhiều lần trong lúc huấn luyện

```
x = np.asarray(x_train) #data, shape (num_image,100,100,3)
y = np.asarray(y_train) #label,shape (num_image,6)
print(len(x))
x_train,y_train = shuffle_data(x,y)
with h5py.File(args.output,'w') as F:
    F.create_dataset('x_train',data = x_train)
    F.create_dataset('y_train',data = y_train)
```

{% include message.html type="is-info" content="Để đơn giản, chỉ cần chạy script dưới đây" %}

```
python3 make_data_and_preprocessing/make_data/make_data/make_data.py --working . --output train_data_100.h5
```

Sau khi chạy xong, ta có file `train_data_100.h5` là file lưu dataset trực tiếp dưới dạng numpy array, tiện để load lên dùng lại nhiều lần trong lúc train. **Lưu ý, với các dataset cực lớn thì lưu trực tiếp vào h5py sẽ không có lợi, thậm chí không chạy được vì không đủ RAM, khi đó ta sẽ dùng các cách khác để load dataset theo từng phần nhỏ trong lúc huấn luyện**

---

## 3. Huấn luyện

### Cài đặt môi trường

{% include message.html type="is-info" content="Môi trường mình dùng là Google Colabotory free, cũng đủ dùng với dataset nhỏ và thời gian train ngắn như bài này. Sau khi làm xong các bước chuẩn bị dữ liệu, hãy copy file <code>[tên_của_dataset].h5</code> lên Google Drive, tạo một notebook google colabotory và bắt đầu làm các bước trong notebook dưới đây" %}


* Gắn drive làm ổ cứng, nơi lưu dữ liệu của notebook . Hiện tại thì đã có giao diện ở thanh công cụ bên tay trái screen, nếu thích trực quan thì các bạn hãy dùng thanh công cụ đó thay cho các dòng code mount disk bên dưới.


```python
from google.colab import drive
drive.mount('/content/drive')
```

    Mounted at /content/drive


* Import các thư viện cần thiết, chuyển working directory sang nơi chứa file <dataset>.h5


```python
import tensorflow as tf
import keras
import cv2
import numpy as np
import os
import h5py
from matplotlib import pyplot as plt
import time
os.chdir('./drive/My Drive/GG_colab/CDTproject') #hãy thay đổi theo nơi lưu <dataset>.h5
```




* Load dataset từ file h5py


```python
with h5py.File("train_data_100_18-02.h5",'r') as F:
    x_train = np.array(F.get("x_train"))
    y_train = np.array(F.get("y_train"))
    x_test = np.array(F.get("x_test"))
    y_test = np.array(F.get("y_test"))

print(x_train.shape,y_train.shape)
print(x_test.shape,y_test.shape)

```

    (2689, 100, 100, 3) (2689, 6)
    (350, 100, 100, 3) (350, 6)


### Tạo kiến trúc của mô hình CNN

{% include message.html type="is-info" content="Ở đây dùng các layer Convolutional bình thường kết hợp với Bottleneck residual block của MobileNet." %}

* Code khối bottleneck residual block


```python
def bottleneck_res_block(block_input,factor):
  ###expansion convolution layer
  x = keras.layers.Conv2D(int(factor)*int(block_input.shape[3]),(1,1))(block_input) 
  x = keras.layers.BatchNormalization()(x)
  x = keras.layers.ReLU(max_value = 6)(x)
  
  ###depthwise convolution layer
  x = keras.layers.DepthwiseConv2D((3,3),padding = 'same')(x)
  x = keras.layers.BatchNormalization()(x)
  x = keras.layers.ReLU(max_value = 6)(x)
  
  ###projection convolution layer
  x = keras.layers.Conv2D(int(int(x.shape[3])/int(factor)),(1,1))(x)
  x = keras.layers.BatchNormalization()(x)
  #x = keras.layers.ReLU(max_value = 6)(x)
  
  #Residual connect
  x = keras.layers.Add()([x,block_input])
  x = keras.layers.Dropout(rate = 0.5)(x)
  
  return x
```

* Đưa vào mô hình, tạo ra model hoàn chỉnh để train


```python
def Tomato_model(input_shape):
  x_input = keras.layers.Input(input_shape)
  
  x = keras.layers.Conv2D(32,(1,1),padding = 'same')(x_input)
  x = keras.layers.BatchNormalization()(x)
  x = keras.layers.MaxPool2D((2,2))(x)
  x = keras.layers.ReLU(max_value = 6)(x)
  x = keras.layers.Dropout(rate = 0.2)(x)
  x = bottleneck_res_block(x,2)
  x = keras.layers.AveragePooling2D((2,2))(x)
  x = bottleneck_res_block(x,2)
  x = keras.layers.GlobalAveragePooling2D()(x)
  x = keras.layers.Dense(6,activation = 'softmax')(x)
  
  model = keras.models.Model(inputs = x_input, outputs = x)
  
  return model
```

* Thông số của mô hình


```python
model = Tomato_model((100,100,3))
model.summary()
```

    Model: "model"
    __________________________________________________________________________________________________
    Layer (type)                    Output Shape         Param #     Connected to                     
    ==================================================================================================
    input_1 (InputLayer)            [(None, 100, 100, 3) 0                                            
    __________________________________________________________________________________________________
    conv2d (Conv2D)                 (None, 100, 100, 32) 128         input_1[0][0]                    
    __________________________________________________________________________________________________
    batch_normalization (BatchNorma (None, 100, 100, 32) 128         conv2d[0][0]                     
    __________________________________________________________________________________________________
    max_pooling2d (MaxPooling2D)    (None, 50, 50, 32)   0           batch_normalization[0][0]        
    __________________________________________________________________________________________________
    re_lu (ReLU)                    (None, 50, 50, 32)   0           max_pooling2d[0][0]              
    __________________________________________________________________________________________________
    dropout (Dropout)               (None, 50, 50, 32)   0           re_lu[0][0]                      
    __________________________________________________________________________________________________
    conv2d_1 (Conv2D)               (None, 50, 50, 64)   2112        dropout[0][0]                    
    __________________________________________________________________________________________________
    batch_normalization_1 (BatchNor (None, 50, 50, 64)   256         conv2d_1[0][0]                   
    __________________________________________________________________________________________________
    re_lu_1 (ReLU)                  (None, 50, 50, 64)   0           batch_normalization_1[0][0]      
    __________________________________________________________________________________________________
    depthwise_conv2d (DepthwiseConv (None, 50, 50, 64)   640         re_lu_1[0][0]                    
    __________________________________________________________________________________________________
    batch_normalization_2 (BatchNor (None, 50, 50, 64)   256         depthwise_conv2d[0][0]           
    __________________________________________________________________________________________________
    re_lu_2 (ReLU)                  (None, 50, 50, 64)   0           batch_normalization_2[0][0]      
    __________________________________________________________________________________________________
    conv2d_2 (Conv2D)               (None, 50, 50, 32)   2080        re_lu_2[0][0]                    
    __________________________________________________________________________________________________
    batch_normalization_3 (BatchNor (None, 50, 50, 32)   128         conv2d_2[0][0]                   
    __________________________________________________________________________________________________
    add (Add)                       (None, 50, 50, 32)   0           batch_normalization_3[0][0]      
                                                                     dropout[0][0]                    
    __________________________________________________________________________________________________
    dropout_1 (Dropout)             (None, 50, 50, 32)   0           add[0][0]                        
    __________________________________________________________________________________________________
    average_pooling2d (AveragePooli (None, 25, 25, 32)   0           dropout_1[0][0]                  
    __________________________________________________________________________________________________
    conv2d_3 (Conv2D)               (None, 25, 25, 64)   2112        average_pooling2d[0][0]          
    __________________________________________________________________________________________________
    batch_normalization_4 (BatchNor (None, 25, 25, 64)   256         conv2d_3[0][0]                   
    __________________________________________________________________________________________________
    re_lu_3 (ReLU)                  (None, 25, 25, 64)   0           batch_normalization_4[0][0]      
    __________________________________________________________________________________________________
    depthwise_conv2d_1 (DepthwiseCo (None, 25, 25, 64)   640         re_lu_3[0][0]                    
    __________________________________________________________________________________________________
    batch_normalization_5 (BatchNor (None, 25, 25, 64)   256         depthwise_conv2d_1[0][0]         
    __________________________________________________________________________________________________
    re_lu_4 (ReLU)                  (None, 25, 25, 64)   0           batch_normalization_5[0][0]      
    __________________________________________________________________________________________________
    conv2d_4 (Conv2D)               (None, 25, 25, 32)   2080        re_lu_4[0][0]                    
    __________________________________________________________________________________________________
    batch_normalization_6 (BatchNor (None, 25, 25, 32)   128         conv2d_4[0][0]                   
    __________________________________________________________________________________________________
    add_1 (Add)                     (None, 25, 25, 32)   0           batch_normalization_6[0][0]      
                                                                     average_pooling2d[0][0]          
    __________________________________________________________________________________________________
    dropout_2 (Dropout)             (None, 25, 25, 32)   0           add_1[0][0]                      
    __________________________________________________________________________________________________
    global_average_pooling2d (Globa (None, 32)           0           dropout_2[0][0]                  
    __________________________________________________________________________________________________
    dense (Dense)                   (None, 6)            198         global_average_pooling2d[0][0]   
    ==================================================================================================
    Total params: 11,398
    Trainable params: 10,694
    Non-trainable params: 704
    __________________________________________________________________________________________________


### Huấn luyện mô hình vừa khởi tạo

* Khởi tạo optimizer, loss và metrics cho model, bắt đầu huấn luyện mô hình theo tỉ lệ train/valid là 8:2 


```python
model.compile(optimizer ='adam', loss = 'categorical_crossentropy',metrics = ['accuracy'])
```

* Chuyển param mode trong dòng `checkpoint` thành `best` nếu chỉ muốn lưu model có độ chính xác trên tập validation lớn nhất
* `filepath` là format đường dẫn của những model lưu xuống trong khi train, có thể thay đổi tùy ý


```python
filepath="trained_model/size_100_18-02/weights.100.{epoch:02d}-{val_accuracy:.2f}.h5"
checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=0, mode='auto') # change 'auto' to 'best' if needed
callbacks_list = [checkpoint]
model.fit(x = x_train, y = y_train, epochs = 300,validation_split=0.2,callbacks = callbacks_list)
```

    Epoch 1/300
    60/60 [==============================] - 36s 43ms/step - loss: 1.3388 - accuracy: 0.5213 - val_loss: 1.8314 - val_accuracy: 0.1814
    Epoch 2/300
    60/60 [==============================] - 2s 31ms/step - loss: 0.5242 - accuracy: 0.8367 - val_loss: 1.9896 - val_accuracy: 0.1392
    Epoch 3/300
    60/60 [==============================] - 2s 32ms/step - loss: 0.4235 - accuracy: 0.8678 - val_loss: 2.3452 - val_accuracy: 0.2785
    Epoch 4/300
    60/60 [==============================] - 2s 32ms/step - loss: 0.3040 - accuracy: 0.9059 - val_loss: 3.0306 - val_accuracy: 0.1730
    Epoch 5/300
    60/60 [==============================] - 2s 32ms/step - loss: 0.2901 - accuracy: 0.9038 - val_loss: 3.3912 - val_accuracy: 0.1688
    Epoch 6/300
    60/60 [==============================] - 2s 32ms/step - loss: 0.2203 - accuracy: 0.9297 - val_loss: 3.3681 - val_accuracy: 0.2384
    Epoch 7/300
    60/60 [==============================] - 2s 32ms/step - loss: 0.1919 - accuracy: 0.9476 - val_loss: 1.9309 - val_accuracy: 0.4873
    Epoch 8/300
    60/60 [==============================] - 2s 32ms/step - loss: 0.1990 - accuracy: 0.9358 - val_loss: 1.1571 - val_accuracy: 0.5992
    Epoch 9/300
    60/60 [==============================] - 2s 32ms/step - loss: 0.2124 - accuracy: 0.9262 - val_loss: 0.6477 - val_accuracy: 0.7637
    .
    .
    .
    Epoch 295/300
    60/60 [==============================] - 2s 33ms/step - loss: 0.0095 - accuracy: 0.9988 - val_loss: 0.0247 - val_accuracy: 0.9873
    Epoch 296/300
    60/60 [==============================] - 2s 33ms/step - loss: 0.0207 - accuracy: 0.9918 - val_loss: 0.0763 - val_accuracy: 0.9852
    Epoch 297/300
    60/60 [==============================] - 2s 32ms/step - loss: 0.0116 - accuracy: 0.9959 - val_loss: 0.0303 - val_accuracy: 0.9873
    Epoch 298/300
    60/60 [==============================] - 2s 32ms/step - loss: 0.0567 - accuracy: 0.9814 - val_loss: 0.1111 - val_accuracy: 0.9599
    Epoch 299/300
    60/60 [==============================] - 2s 33ms/step - loss: 0.0396 - accuracy: 0.9879 - val_loss: 0.3911 - val_accuracy: 0.8797
    Epoch 300/300
    60/60 [==============================] - 2s 33ms/step - loss: 0.0135 - accuracy: 0.9980 - val_loss: 0.1140 - val_accuracy: 0.9662





    <tensorflow.python.keras.callbacks.History at 0x7fc220082c50>



* Vẽ loss và accuracy của quá trình huấn luyện lên đồ thị để quan sát sự thay đổi và đưa ra đánh giá


```python
"""
Accuracy
"""
#Hàm savgol_filter để làm đồ thị mượt hơn, có thể dùng cho đẹp
from scipy.signal import savgol_filter
from numpy import log as ln
from numpy import log10 as log
history = model.history
acc = savgol_filter(history.history['acc'],51,10)
val_acc = savgol_filter(history.history['val_acc'],51,10)
plt.rcParams["figure.figsize"] = (9,6)
plt.plot(acc)
plt.plot(val_acc)
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'valid'], loc='upper left')
plt.show()
```
<div class="has-text-centered">
    <img src="/img/DATN/output_21_0.png">
</div>

```python
"""
Loss
"""
# summarize history for loss
plt.plot(history.history['loss'][0:299])
plt.plot(history.history['val_loss'][0:299])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
```

<div class="has-text-centered">
    <img src="/img/DATN/output_22_0.png">
</div>
    
    

* Sau khi huấn luyện và đánh giá xong, chọn mô hình tốt nhất để triển khai lên hệ thống phân loại, model file sẽ có dạng `<weight>.h5`

---
## 4. Triển khai

{% include message.html type="is-info" content="Hệ thống mình deploy lên là Raspberry Pi 3B+ và đạt được FPS vào khoảng 7FPS khi chạy thời gian thực" %}

### Cài đặt:

* Sử dụng lại repo Github đã clone lúc [chuẩn bị dữ liệu](#2-chuẩn-bị-dữ-liệu)

Repo: `https://github.com/phattruongai/Tomato-classification-system.git`

Chuyển đến thư mục `Classification`:

```
cd Classification
```

* Kết nối USB webcam tới máy tính hoặc Raspberry Pi

<div class="has-text-centered">
    <img src="/img/DATN/pi.jpg">
</div>

### Chạy code thời gian thực:

* Chạy:

```
python3 test_script/test/test/test.py --workingdir . --modelpath <your_model_path>
```

* Kết quả sẽ được hiển thị lên màn hình như hình dưới:

<div class="has-text-centered">
    <img src="/img/DATN/display.png">
</div>

### Giải thích code:

* Load model và lấy session hiện tại:

```python
#change dir and load model
os.chdir(args.workingdir)
model = load_model(args.modelpath) 
#get session of model to use in gradCAM computing
sess = tf.keras.backend.get_session()
```

* Gọi hàm `test_vid` với param = "camera" để chạy real time với các frame lấy được từ webcam
{% include message.html type="is-info" content="Hàm <code>test_vid</code> nằm trong file <code>utils.py</code>" %}

```python
#run real time process
test_vid("camera",model,sess)
```

* Hàm `test_vid` bao gồm các công việc:

    * Khởi tạo static graph cho việc tính toán grad-CAM:
    ```python
    arg = grad_cam_generate(model,"add_2",6)
    ```
    
    * Đọc frame từ webcam, resize và normalize:
    ```python
    # Read frame from camera
    _,frame = cap.read()
    # Preprocessing
    img = cv2.resize(frame[40:410,140:500,:],(100,100))/255.
    ```
    
    * Chạy model:
    ```python
    # Let model do the prediction
    predictions = model.predict(np.expand_dims(img,0))
    prob = np.max(predictions)
    predicted_class = np.argmax(predictions)
    print("     ",predicted_class)
    ```
    
    * Dùng output của model để tính grad-CAM, tạo heatmap:
    ```python
    # Generate the grad-CAM heatmap to visualize
    cam,heatmap,full_img = grad_cam(model, predicted_class,sess, img, arg)
    font = cv2.FONT_HERSHEY_SIMPLEX
    img = np.concatenate((full_img,cam), axis = 1) # create 2 views, 1 for original image, 1 for images with grad-CAM heatmap
    img = cv2.resize(img,disp_size) # resize to the fixed size to visualize
    end = time.time() # mark the finishing time
    fps = 1/(end - start) # compute FPS
    ```
    

